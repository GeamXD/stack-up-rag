# # -*- coding: utf-8 -*-
# """hack.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1neWCwnP8EWU4HThFVJpSpEsrgUSHG_cn
# """

# !pip install -r requirements.txt

# # !mkdir data
# # !mv stack-help_data.json data/

# import json
# import pandas as pd
# import datetime
# import re
# import string
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np

# def wrangle_data(filepath: str) -> pd.DataFrame:
#     """
#     This function reads in the data from the filepath, cleans it and returns a pandas dataframe

#     Args:
#     filepath: str: a path to the json file

#     Returns:
#     df: pd.DataFrame: a pandas dataframe with the cleaned data

#     """
#     # load data
#     with open(filepath, 'r', encoding='utf-8') as f:
#         data = json.load(f)

#     # Get article body, urls and titles
#     doc = {
#         'article_title': data['article_link_title'],
#         'article_links': data['article_links'],
#         'article_body': data['article_body'],
#     }


#     # Store the data in a pandas dataframe
#     df = pd.DataFrame(doc)


#     ## Clean article title
#     # Remove the forward-slash character
#     df['article_title_cleaned'] = df['article_title'].str.replace(r"\/","")

#     # Remove punctuation
#     df['article_title_cleaned'] = df['article_title_cleaned'].str.translate(string.punctuation)

#     # Remove digits
#     df['article_title_cleaned'] = df['article_title_cleaned'].str.replace(r"\d+","")

#     # Remove running spaces
#     df['article_title_cleaned'] = df['article_title_cleaned'].str.replace(r"\s{2,}","")

#     # Make the text lowercase
#     df['article_title_cleaned'] = df['article_title_cleaned'].str.lower()

#     ## Clean article body

#     # Remove the forward-slash character
#     df['article_body_cleaned'] = df['article_body'].str.replace(r"\/","")

#     # Remove punctuation
#     df['article_body_cleaned'] = df['article_body_cleaned'].str.translate(string.punctuation)

#     # Remove digits
#     df['article_body_cleaned'] = df['article_body_cleaned'].str.replace(r"\d+","")

#     # Remove running spaces
#     df['article_body_cleaned'] = df['article_body_cleaned'].str.replace(r"\s{2,}","")

#     # Make the text lowercase
#     df['article_body_cleaned'] = df['article_body_cleaned'].str.lower()

#     # Remove double white space
#     df['article_body_cleaned'] = df['article_body_cleaned'].str.replace('  ', '')

#     return df


# doc_df = wrangle_data('data/stack-help_data.json')

# doc_df.head()

# # Basics
# import os
# # from dotenv import load_dotenv

# # LangChain Training
# # LLM
# from langchain.llms import OpenAI

# # Document Loader
# from langchain.document_loaders import PyPDFLoader

# # Splitter
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# # Tokenizer
# from transformers import GPT2TokenizerFast

# # Embedding
# from langchain.embeddings import OpenAIEmbeddings

# # Vector DataBase
# from langchain.vectorstores import FAISS, Pinecone # for the vector database part -- FAISS is local and temporal, Pinecone is cloud-based and permanent.

# # Chains
# #from langchain.chains.question_answering import load_qa_chain
# #from langchain.chains import ConversationalRetrievalChain

# doc_df.info()

# # Import DataFrameLoader
# from langchain.document_loaders import DataFrameLoader

# new_df = doc_df[['article_title_cleaned', 'article_body_cleaned', 'article_links']]
# new_df.rename(columns={'article_title_cleaned': 'title', 'article_body_cleaned': 'page_content', 'article_links': 'urls'}, inplace=True)

# # help(DataFrameLoader)
# # The page content column is 'movie_description'
# docs = DataFrameLoader(
#     new_df,
#     page_content_column="page_content",
# ).load()

# # Print the first 3 documents and the number of documents
# docs[:1]
# # display(f"Number of documents: {len(docs)}")

# # Import os and pinecone
# import pinecone

# # Set the pinecone api key from the environment variable. Assign to api_key.
# api_key=os.getenv("PINECONE_API_KEY")

# # Initialize pinecone using the `PINECONE_API_KEY` variable.
# pc = pinecone.Pinecone(api_key)

# # Import os and pinecone
# # Use this index name
# index_name = "stackragapp"

# # List the names of available indexes. Assign to existing_index_names.
# existing_index_names = [idx.name for idx in pc.list_indexes().indexes]

# # First check that the given index does not exist yet
# if index_name not in existing_index_names:
#     # Create the 'imbd-movies' index with cosine metric, 1536 dims, serverless spec: aws in us-east-1
#     pc.create_index(
#         name=index_name,
#         metric='cosine',
#         dimension=768,
#         spec=pinecone.ServerlessSpec(cloud="aws", region="us-east-1")
#     )

# # from pinecone import Pinecone, ServerlessSpec

# # pc = Pinecone(api_key="13f37079-ed89-4ec2-b987-4a2dc1b7a7c1")

# # pc.create_index(
# #     name="stack_rag",
# #     dimension=786, # Replace with your model dimensions
# #     metric="cosine", # Replace with your model metric
# #     spec=ServerlessSpec(
# #         cloud="aws",
# #         region="us-east-1"
# #     )
# # )

# from langchain_together.embeddings import TogetherEmbeddings

# # Create the embeddings object
# embeddings = TogetherEmbeddings(model="togethercomputer/m2-bert-80M-8k-retrieval")

# # From the langchain_pinecone package, import PineconeVectorStore
# from langchain_pinecone import PineconeVectorStore

# # Create an index from its name
# index = pc.Index(index_name)

# # Count the number of vectors in the index
# n_vectors = index.describe_index_stats()['total_vector_count']
# print(f"There are {n_vectors} vectors in the index already.")

# # Check if there is already some data in the index on Pinecone
# if n_vectors > 0:
#     # If there is, get the documents to search from the index. Assign to docsearch.
#     docsearch = PineconeVectorStore.from_existing_index(index_name, embeddings)
# else:
#     # If not, fill the index from the documents and return those docs to assign to docsearch
#     docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)

